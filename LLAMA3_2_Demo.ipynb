{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Text Image Analayzer Gradio app using LLAMA3.2-11B-Vision model.This demo requires A100 GPU"
      ],
      "metadata": {
        "id": "wxOFKuzaaFEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -U transformers bitsandbytes accelerate peft -q"
      ],
      "metadata": {
        "id": "ytCOXNqNKsQJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install gradio -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JxKTjOGPzvl",
        "outputId": "ebd03fbd-0d75-4df9-cbae-469de910b6c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
        "\n",
        "# Load the Llama 3.2 Vision Model\n",
        "def load_llama_model():\n",
        "    model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n",
        "\n",
        "    # Load model and processor\n",
        "    model = MllamaForConditionalGeneration.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "    return model, processor\n",
        "\n",
        "# Function to generate predictions for text and image\n",
        "def process_input(text, image=None):\n",
        "    model, processor = load_llama_model()\n",
        "\n",
        "    if image:\n",
        "        # If an image is uploaded, process it as a PIL Image object\n",
        "        vision_input = image.convert(\"RGB\").resize((224, 224))\n",
        "\n",
        "        prompt = f\"<|image|><|begin_of_text|>{text}\"\n",
        "\n",
        "        # Process image and text together\n",
        "        inputs = processor(vision_input, prompt, return_tensors=\"pt\").to(model.device)\n",
        "    else:\n",
        "        # If no image is uploaded, just process the text\n",
        "        prompt = f\"<|begin_of_text|>{text}\"\n",
        "        inputs = processor(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate output from the model\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "    # Decode the output to return a readable text\n",
        "    decoded_output = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return decoded_output\n",
        "\n",
        "# Gradio Interface Setup\n",
        "def demo():\n",
        "    # Define Gradio input and output components\n",
        "    text_input = gr.Textbox(label=\"Text Input\", placeholder=\"Enter text here\", lines=5)\n",
        "\n",
        "    # Use type=\"pil\" to work with PIL Image objects\n",
        "    image_input = gr.Image(label=\"Upload an Image\", type=\"pil\")\n",
        "\n",
        "    output = gr.Textbox(label=\"Model Output\", lines=5)\n",
        "\n",
        "    # Define the interface layout\n",
        "    interface = gr.Interface(\n",
        "        fn=process_input,\n",
        "        inputs=[text_input, image_input],\n",
        "        outputs=output,\n",
        "        title=\"Llama 3.2 Multimodal Text-Image Analyzer\",\n",
        "        description=\"Upload an image and/or provide text for analysis using the Llama 3.2 Vision Model.\"\n",
        "    )\n",
        "\n",
        "    # Launch the demo\n",
        "    interface.launch()\n",
        "\n",
        "# Run the demo\n",
        "if __name__ == \"__main__\":\n",
        "    demo()"
      ],
      "metadata": {
        "id": "fmKbretNJsEo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "a9438e05-2603-441b-cb56-684bb663b8af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://0fe7a691d7eb981c7d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0fe7a691d7eb981c7d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xkjF2T_QPyOL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}